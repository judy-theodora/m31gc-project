{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c06578c5",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fd92fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "238022ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC #???????????????//\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr, kendalltau\n",
    "\n",
    "import astropy.table\n",
    "from astropy.table import QTable, join\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16bd66a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_cat = pd.read_csv('./catdata/master_catalog_jan_2023.csv')\n",
    "cat_files = ['cat1_50.pk','cat51_100.pk','cat101_150.pk','cat151_200.pk','cat201_235.pk',\n",
    "             'cat236_257.pk','cat258_279.pk','cat280_320.pk','cat321_360.pk','cat361_406.pk']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3340473d",
   "metadata": {},
   "source": [
    "### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35df8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select most recent training data\n",
    "train_file = 'training_data_0802.pk' # training data with 3 classes\n",
    "train_file = 'training_data_1702.pk' # training data with only gcs and galaxies\n",
    "#train_file = 'training_data_1902_with_stars.pk' # training data with gcs galaxies and stars, classed as 'gc' and 'non-gc'\n",
    "\n",
    "# load training data and filter out stars\n",
    "with open(f'./pickle/training_data/{train_file}','rb') as f:\n",
    "    training_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "295a6998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj_id</th>\n",
       "      <th>class</th>\n",
       "      <th>i</th>\n",
       "      <th>g</th>\n",
       "      <th>di</th>\n",
       "      <th>dg</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>field</th>\n",
       "      <th>pdidx</th>\n",
       "      <th>rbcidx</th>\n",
       "      <th>nearby</th>\n",
       "      <th>i-g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HM33-A</td>\n",
       "      <td>gc</td>\n",
       "      <td>22.424000</td>\n",
       "      <td>22.940001</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.017</td>\n",
       "      <td>23.923733</td>\n",
       "      <td>28.821186</td>\n",
       "      <td>5</td>\n",
       "      <td>39800</td>\n",
       "      <td>2647</td>\n",
       "      <td>49</td>\n",
       "      <td>-0.516001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C30</td>\n",
       "      <td>galaxy</td>\n",
       "      <td>18.049000</td>\n",
       "      <td>19.500999</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>12.105896</td>\n",
       "      <td>29.267633</td>\n",
       "      <td>11</td>\n",
       "      <td>118854</td>\n",
       "      <td>2431</td>\n",
       "      <td>24</td>\n",
       "      <td>-1.452000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LAMOST-C22</td>\n",
       "      <td>galaxy</td>\n",
       "      <td>17.628000</td>\n",
       "      <td>19.153000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>11.738621</td>\n",
       "      <td>29.693506</td>\n",
       "      <td>11</td>\n",
       "      <td>24692</td>\n",
       "      <td>2319</td>\n",
       "      <td>17</td>\n",
       "      <td>-1.525000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HM33-B</td>\n",
       "      <td>gc</td>\n",
       "      <td>19.538000</td>\n",
       "      <td>20.386000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>24.008787</td>\n",
       "      <td>29.963625</td>\n",
       "      <td>13</td>\n",
       "      <td>43246</td>\n",
       "      <td>2648</td>\n",
       "      <td>45</td>\n",
       "      <td>-0.848000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LAMOST-C18</td>\n",
       "      <td>galaxy</td>\n",
       "      <td>17.177999</td>\n",
       "      <td>18.388000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>23.842129</td>\n",
       "      <td>29.552473</td>\n",
       "      <td>14</td>\n",
       "      <td>122860</td>\n",
       "      <td>2644</td>\n",
       "      <td>12</td>\n",
       "      <td>-1.210001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>PA-N185</td>\n",
       "      <td>gc</td>\n",
       "      <td>20.188000</td>\n",
       "      <td>21.688999</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.007</td>\n",
       "      <td>9.578100</td>\n",
       "      <td>48.367985</td>\n",
       "      <td>398</td>\n",
       "      <td>256376</td>\n",
       "      <td>306</td>\n",
       "      <td>106</td>\n",
       "      <td>-1.500999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>FJJ-V</td>\n",
       "      <td>gc</td>\n",
       "      <td>17.434999</td>\n",
       "      <td>18.451000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>9.806167</td>\n",
       "      <td>48.384743</td>\n",
       "      <td>398</td>\n",
       "      <td>168423</td>\n",
       "      <td>372</td>\n",
       "      <td>156</td>\n",
       "      <td>-1.016001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>FJJ-VII</td>\n",
       "      <td>gc</td>\n",
       "      <td>19.523001</td>\n",
       "      <td>20.520000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>9.826713</td>\n",
       "      <td>48.384266</td>\n",
       "      <td>398</td>\n",
       "      <td>168594</td>\n",
       "      <td>377</td>\n",
       "      <td>163</td>\n",
       "      <td>-0.997000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>PA-N147-3</td>\n",
       "      <td>gc</td>\n",
       "      <td>19.910000</td>\n",
       "      <td>20.820999</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>8.542029</td>\n",
       "      <td>49.044243</td>\n",
       "      <td>402</td>\n",
       "      <td>101369</td>\n",
       "      <td>91</td>\n",
       "      <td>36</td>\n",
       "      <td>-0.910999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>PA-N147-2</td>\n",
       "      <td>gc</td>\n",
       "      <td>17.752001</td>\n",
       "      <td>18.606001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>8.430446</td>\n",
       "      <td>48.645897</td>\n",
       "      <td>402</td>\n",
       "      <td>163343</td>\n",
       "      <td>84</td>\n",
       "      <td>60</td>\n",
       "      <td>-0.854000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1334 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          obj_id   class          i          g     di     dg         ra  \\\n",
       "0         HM33-A      gc  22.424000  22.940001  0.026  0.017  23.923733   \n",
       "1            C30  galaxy  18.049000  19.500999  0.001  0.002  12.105896   \n",
       "2     LAMOST-C22  galaxy  17.628000  19.153000  0.001  0.001  11.738621   \n",
       "3         HM33-B      gc  19.538000  20.386000  0.003  0.003  24.008787   \n",
       "4     LAMOST-C18  galaxy  17.177999  18.388000  0.001  0.001  23.842129   \n",
       "...          ...     ...        ...        ...    ...    ...        ...   \n",
       "1329     PA-N185      gc  20.188000  21.688999  0.005  0.007   9.578100   \n",
       "1330       FJJ-V      gc  17.434999  18.451000  0.001  0.001   9.806167   \n",
       "1331     FJJ-VII      gc  19.523001  20.520000  0.003  0.004   9.826713   \n",
       "1332   PA-N147-3      gc  19.910000  20.820999  0.004  0.004   8.542029   \n",
       "1333   PA-N147-2      gc  17.752001  18.606001  0.001  0.001   8.430446   \n",
       "\n",
       "            dec  field   pdidx  rbcidx  nearby       i-g  \n",
       "0     28.821186      5   39800    2647      49 -0.516001  \n",
       "1     29.267633     11  118854    2431      24 -1.452000  \n",
       "2     29.693506     11   24692    2319      17 -1.525000  \n",
       "3     29.963625     13   43246    2648      45 -0.848000  \n",
       "4     29.552473     14  122860    2644      12 -1.210001  \n",
       "...         ...    ...     ...     ...     ...       ...  \n",
       "1329  48.367985    398  256376     306     106 -1.500999  \n",
       "1330  48.384743    398  168423     372     156 -1.016001  \n",
       "1331  48.384266    398  168594     377     163 -0.997000  \n",
       "1332  49.044243    402  101369      91      36 -0.910999  \n",
       "1333  48.645897    402  163343      84      60 -0.854000  \n",
       "\n",
       "[1334 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd4eb30",
   "metadata": {},
   "source": [
    "### Add i-g to training data (17/02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "dfdacdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['i-g'] = training_data['i']-training_data['g']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d35d779",
   "metadata": {},
   "source": [
    "# Training Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d22fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cat(field):\n",
    "    bounds = [50,100,150,200,235,257,279,320,360,406]\n",
    "    for b in range(len(bounds)):\n",
    "        if field <= bounds[b]:\n",
    "            to_load = cat_files[b]\n",
    "            break\n",
    "    with open(f'./pickle/{to_load}','rb') as f:\n",
    "        catalogue = pickle.load(f)\n",
    "    return catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0890fddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23.01.26 18:29\n",
    "def generate_training_data(matches, crowding=300) -> pd.DataFrame:\n",
    "    cat = load_cat(1)\n",
    "    \n",
    "    columns = ['obj_id','class','i','g','di','dg','ra','dec','field','pdidx','rbcidx','nearby']\n",
    "    values = []\n",
    "    object_ids = []\n",
    "    \n",
    "    #TEMP\n",
    "    crowded_objects = []\n",
    "    \n",
    "    \n",
    "    for field in matches: # iterate through each field ID\n",
    "        working_field = matches[field] # take the list of matches e.g. working_field = [(166727, 2642), (159637, 2646)]\n",
    "        if field not in cat: # load the correct catalogue\n",
    "            cat = load_cat(field)\n",
    "        for m in working_field: # iterate through each match (a tuple) and grab values from catalogues\n",
    "            \n",
    "            if m[2] > crowding: # testing\n",
    "                crowded_objects.append(m[1:])\n",
    "                continue\n",
    "            \n",
    "            obj_id = master_cat.loc[m[1]].ID\n",
    "            class_ = master_cat.loc[m[1]].CLASS\n",
    "            \n",
    "            if obj_id in object_ids: continue # if we've already added the object then skip\n",
    "            else: object_ids.append(obj_id)   # else add it to the list of ids\n",
    "            \n",
    "            if class_ == 1: class_str = 'gc' # convert class numbers into strings\n",
    "            elif class_ == 8: class_str = 'gc' # include extended clusters\n",
    "            elif class_ == 4: class_str = 'galaxy'\n",
    "           # elif class_ == 6: class_str = 'star'\n",
    "            else: continue # skip non-gc/gal objects\n",
    "            \n",
    "            # collect required data\n",
    "            row = cat[field][m[0]]\n",
    "            ra = row['RA']\n",
    "            dec = row['Dec']\n",
    "            g = row['g']\n",
    "            i = row['i']\n",
    "            dg = row['dg']\n",
    "            di = row['di']\n",
    "            \n",
    "            values.append([obj_id,class_str,i,g,di,dg,ra,dec,field,m[0],m[1],m[2]])\n",
    "    \n",
    "    training_data_dict = dict(zip(columns,zip(*values))) # zip values and columns together into a dict (columns as keys)\n",
    "    training_data_df = pd.DataFrame(training_data_dict) # put into pd Dataframe\n",
    "    return training_data_df, crowded_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39fbceb",
   "metadata": {},
   "source": [
    "#### Generate training data from object matches (17/02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3fada3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./pickle/matches/matches_delta005_1702.pk','rb') as f:\n",
    "    obj_mat = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a1831b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cat1_50.pk ...\n",
      "Loading cat51_100.pk ...\n",
      "Loading cat101_150.pk ...\n",
      "Loading cat151_200.pk ...\n",
      "Loading cat201_235.pk ...\n",
      "Loading cat236_257.pk ...\n",
      "Loading cat258_279.pk ...\n",
      "Loading cat280_320.pk ...\n",
      "Loading cat321_360.pk ...\n",
      "Loading cat361_406.pk ...\n"
     ]
    }
   ],
   "source": [
    "new_training_data, crowded_obj = generate_training_data(obj_mat,crowding=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "68ad9a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./pickle/training_data/training_data_1702.pk','wb') as f:\n",
    "    pickle.dump(new_training_data,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5198aec3",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bfbf62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_correlations(pred,true):\n",
    "    correlations = {}\n",
    "    correlations['mse'] = mean_squared_error(pred,true)\n",
    "    correlations['ktau'] = kendalltau(pred,true)[0]\n",
    "    correlations['pval-ktau'] = kendalltau(pred,true)[1]\n",
    "    correlations['pearsonr'] = pearsonr(pred,true)[0]\n",
    "    correlations['pval-pearsonr'] = pearsonr(pred,true)[1]\n",
    "    correlations['r2'] = r2_score(true, pred)\n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7d550ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_corr(c): # pretty print output from calc_correlations()\n",
    "    print(f\"\"\"\n",
    "    Mean squared error (RMS): \\t{c['mse']:.5f}\\t({(c['mse']**.5):.5})\n",
    "    Kendall Tau: \\t\\t{c['ktau']:.5}\n",
    "    \\tKtau p-value: \\t\\t{c['pval-ktau']:.5}\n",
    "    Pearson's r: \\t\\t{c['pearsonr']:.5}\n",
    "    \\tPearson's r p-value: \\t{c['pval-pearsonr']:.5}\n",
    "    Coef. of determination \\t{c['r2']:.5}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012ae5bd",
   "metadata": {},
   "source": [
    "# Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b82089e",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1237338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the classifier and return (with optional returning of train and test values)\n",
    "def ranfor(df,train_size=0.8,n_estimators=50,criterion='gini',features=['i','g','i-g'], max_depth=None, max_leaf_nodes=None, min_samples_leaf=1, stats=False, scale=False):\n",
    "    # select features for training\n",
    "    X = df[features]\n",
    "    y = df['class']\n",
    "    # split the data\n",
    "    if scale:\n",
    "        # scale the data\n",
    "        scaler = preprocessing.StandardScaler().fit(X)\n",
    "        X_scaled = scaler.transform(X)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, train_size=train_size) # X_scaled\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size) # X\n",
    "    # train the regressor model\n",
    "    ran_for_class = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                    criterion=criterion, max_leaf_nodes=max_leaf_nodes,\n",
    "                                    min_samples_leaf=min_samples_leaf                                        \n",
    "                                   ).fit(X_train,y_train)\n",
    "    train_pred = ran_for_class.predict(X_train)\n",
    "    test_pred = ran_for_class.predict(X_test)\n",
    "    \n",
    "    acc = ran_for_class.score(X_test,y_test)\n",
    "    \n",
    "    true = y_test.to_numpy()\n",
    "    if stats:\n",
    "        return ran_for_class, test_pred, y_test, train_pred, y_train\n",
    "    else: return ran_for_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37721bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns predictions for a given field, allowing a crowding parameter to filter training values\n",
    "def rf_pred(field,train,crowding=250,n_estimators=50,max_depth=None,max_leaf_nodes=None,min_samples_leaf=1,features=['i','g','i-g'],scale=False):\n",
    "    training_data_ = train[train['nearby'] <= crowding]\n",
    "    cat = load_cat(field)[field]\n",
    "    \n",
    "    # drop rows with high delta g/i values\n",
    "    cat_d = cat[cat['dg']+cat['di'] < 0.05]\n",
    "    # drop stars & saturated points\n",
    "    cat_candidate = cat_d[(cat_d['ig'] == 1) & (cat_d['ii'] == 1)]\n",
    "    cat_candidate['i-g'] = cat_candidate['i']-cat_candidate['g']\n",
    "    \n",
    "    X = cat_candidate[['i','g','i-g']]\n",
    "    X = X.to_pandas()\n",
    "    if scale:\n",
    "        X_scaled = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "        res = ranfor(training_data_,train_size=0.8,n_estimators=n_estimators,criterion='gini',\n",
    "                     features=features, max_depth=max_depth, max_leaf_nodes=max_leaf_nodes,\n",
    "                     min_samples_leaf=1, scale=True).predict(X_scaled)\n",
    "    else:\n",
    "        res = ranfor(training_data_,train_size=0.8,n_estimators=n_estimators,criterion='gini',\n",
    "                     features=features, max_depth=max_depth, max_leaf_nodes=max_leaf_nodes,\n",
    "                     min_samples_leaf=1).predict(X)\n",
    "    \n",
    "    cat_pred = cat_candidate[['RA','Dec','iccd','xg','yg','g','dg','ig','xi','yi','i','di','ii','field']]\n",
    "    cat_pred['pred'] = res\n",
    "    return cat_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da56569e",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aae3f932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc_class(df,train_size=0.8,max_iter=1000,loss='squared_hinge',features=['i','g'], stats=False, scale=False):\n",
    "    # select features for training\n",
    "    X = df[features]\n",
    "    y = df['class']\n",
    "    # split the data\n",
    "    if scale:\n",
    "        # scale the data\n",
    "        scaler = preprocessing.StandardScaler().fit(X)\n",
    "        X_scaled = scaler.transform(X)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, train_size=train_size) # X_scaled\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size) # X\n",
    "    # train the regressor model\n",
    "    svc_classifier = LinearSVC(dual=False,C=0.8,max_iter=max_iter).fit(X_train,y_train)\n",
    "    pred = svc_classifier.predict(X_test)\n",
    "    \n",
    "    true = y_test.to_numpy()\n",
    "    if stats:\n",
    "        return svc_classifier, pred, true\n",
    "    else: return svc_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467d49f0",
   "metadata": {},
   "source": [
    "## MLP  \n",
    "Has trouble with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58b7e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_class(df,train_size=0.8,hidden_layer_sizes=(10,),activation='relu',alpha=0.0001,max_iter=500,features=['i','g'], stats=False, scale=False):\n",
    "    # select features for training\n",
    "    X = df[features]\n",
    "    y = df['class']\n",
    "    # split the data\n",
    "    if scale:\n",
    "        # scale the data\n",
    "        scaler = preprocessing.StandardScaler().fit(X)\n",
    "        X_scaled = scaler.transform(X)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, train_size=train_size) # X_scaled\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size) # X\n",
    "    # train the regressor model\n",
    "    mlp_classifier = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,\n",
    "                                   activation=activation, alpha=alpha, max_iter=max_iter,\n",
    "                                   learning_rate='adaptive',early_stopping=True,\n",
    "                                  ).fit(X_train,y_train)\n",
    "    pred = mlp_classifier.predict(X_test)\n",
    "    true = y_test.to_numpy()\n",
    "    if stats:\n",
    "        return mlp_classifier, pred, true\n",
    "    else: return mlp_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9fa96a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78804065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba5a9ecf",
   "metadata": {},
   "source": [
    "### Functions for varying RF Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea9e8ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add recall accuracy and precision to a dictionary\n",
    "def rf_stats_append(stats, train_test_values, n):\n",
    "    for i in [0,2]:\n",
    "        tn, fp, fn, tp = confusion_matrix(train_test_values[i],train_test_values[i+1],labels=['galaxy','gc']).ravel()\n",
    "        stats[n]['rec'][i>>1].append( tp/(tp+fn) ) # fraction of correctly identified GCs, out of all GCs (1-recall = fraction that were missed)\n",
    "        stats[n]['acc'][i>>1].append( (tp+tn)/(tn+fp+fn+tp) ) # fraction of correctly identified objects\n",
    "        stats[n]['prec'][i>>1].append( tp/(tp+fp) ) # fraction of correctly identified GCs, out of what were thought to be GCs    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1a838e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_vary_trees(n_estimators_range: range, iterations: int, max_leaf_nodes=None, max_depth=None) -> dict:\n",
    "    stats = {}\n",
    "    for n in n_estimators_range:\n",
    "        print(f'calculating for {n} trees...')\n",
    "        stats[n] = {'acc':[[],[]],'prec':[[],[]],'rec':[[],[]]} # ([test],[train])\n",
    "        for i in range(iterations):\n",
    "            train_test_values = ranfor(training_data_nostar,n_estimators=n,\n",
    "                                   max_depth=max_depth, max_leaf_nodes=max_leaf_nodes, stats=True)[1:]\n",
    "            stats = rf_stats_append(stats,train_test_values,n)\n",
    "        for k in stats[n]:\n",
    "            stats[n][k][0] = sum(stats[n][k][0])/iterations\n",
    "            stats[n][k][1] = sum(stats[n][k][1])/iterations\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7811767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_vary_max_leaf_nodes(max_leaf_range: range, iterations: int, n_estimators=50, max_depth=None) -> dict:\n",
    "    stats = {}\n",
    "    for n in max_leaf_range:\n",
    "        print(f'calculating max leaf nodes {n}...')\n",
    "        stats[n] = {'acc':[[],[]],'prec':[[],[]],'rec':[[],[]]} # ([test],[train])\n",
    "        for i in range(iterations):\n",
    "            train_test_values = ranfor(training_data_nostar,n_estimators=n_estimators,\n",
    "                                   max_depth=max_depth, max_leaf_nodes=n, stats=True)[1:]\n",
    "            stats = rf_stats_append(stats,train_test_values,n)\n",
    "        for k in stats[n]:\n",
    "            stats[n][k][0] = sum(stats[n][k][0])/iterations\n",
    "            stats[n][k][1] = sum(stats[n][k][1])/iterations\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99d1bd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_vary_min_samples_leaf(min_samples_range: list, iterations: int, n_estimators=50, max_leaf_nodes=None, max_depth=None) -> dict:\n",
    "    stats = {}\n",
    "    for n in min_samples_range:\n",
    "        print(f'calculating min samples {n}...')\n",
    "        stats[n] = {'acc':[[],[]],'prec':[[],[]],'rec':[[],[]]} # ([test],[train])\n",
    "        for i in range(iterations):\n",
    "            train_test_values = ranfor(training_data_nostar,n_estimators=n_estimators,\n",
    "                                   max_depth=max_depth, max_leaf_nodes=max_leaf_nodes, min_samples_leaf=n, stats=True)[1:]\n",
    "            stats = rf_stats_append(stats,train_test_values,n)\n",
    "        for k in stats[n]:\n",
    "            stats[n][k][0] = sum(stats[n][k][0])/iterations\n",
    "            stats[n][k][1] = sum(stats[n][k][1])/iterations\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7578204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_vary_max_depth(max_depth_range: range, iterations: int, n_estimators=50, max_leaf_nodes=None) -> dict:\n",
    "    stats = {}\n",
    "    for n in max_depth_range:\n",
    "        print(f'calculating max depth {n}...')\n",
    "        stats[n] = {'acc':[[],[]],'prec':[[],[]],'rec':[[],[]]} # ([test],[train])\n",
    "        for i in range(iterations):\n",
    "            train_test_values = ranfor(training_data_nostar,n_estimators=n_estimators,\n",
    "                                   max_depth=n, max_leaf_nodes=max_leaf_nodes, stats=True)[1:]\n",
    "            stats = rf_stats_append(stats,train_test_values,n)\n",
    "        for k in stats[n]:\n",
    "            stats[n][k][0] = sum(stats[n][k][0])/iterations\n",
    "            stats[n][k][1] = sum(stats[n][k][1])/iterations\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cba3a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a plot of the different statistics from a dictionary\n",
    "def plot_stats(stats: dict, xlabel: str):\n",
    "    keys_ = stats.keys()\n",
    "    acc = [stats[k]['acc'] for k in keys_]\n",
    "    prec = [stats[k]['prec'] for k in keys_]\n",
    "    rec = [stats[k]['rec'] for k in keys_]\n",
    "    plt.plot(keys_, acc, label='accuracy')\n",
    "    plt.plot(keys_, prec, label='precision')\n",
    "    plt.plot(keys_, rec, label='recall')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('score')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4dd55c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e0b3b6e",
   "metadata": {},
   "source": [
    "\n",
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "261a366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select most recent training data\n",
    "train_file = 'training_data_0802.pk' # training data with 3 classes\n",
    "train_file = 'training_data_1702.pk' # training data with only gcs and galaxies\n",
    "#train_file = 'training_data_1902_with_stars.pk' # training data with gcs galaxies and stars, classed as 'gc' and 'non-gc'\n",
    "#train_file = 'temp/train_plus_35.pk'\n",
    "#train_file = 'temp/train_plus_148.pk'\n",
    "\n",
    "# load training data and filter out stars\n",
    "with open(f'./pickle/training_data/{train_file}','rb') as f:\n",
    "    training_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "64427163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field 35\n",
      "Filtering...\n",
      "716\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "field = 35\n",
    "n_trees = 50\n",
    "max_depth_ = 9\n",
    "max_leaf_nodes_ = 12\n",
    "min_samples_leaf_ = 10\n",
    "features_ = ['i','g','i-g']\n",
    "\n",
    "predictions_list = []\n",
    "gc_filter = []\n",
    "print(f'Field {field}')\n",
    "for i in range(5):\n",
    "    predictions_list.append( rf_pred(field,training_data,crowding=250,n_estimators=n_trees,max_depth=max_depth_, max_leaf_nodes=max_leaf_nodes_,min_samples_leaf=min_samples_leaf_, features=features_) )\n",
    "print('Filtering...')\n",
    "for i in range(len(predictions_list[0])):\n",
    "    gc_candidate = all([ p[i]['pred']=='gc' for p in predictions_list ])\n",
    "    if gc_candidate: gc_filter.append(True)\n",
    "    else: gc_filter.append(False)\n",
    "gc_candidates = predictions_list[0][gc_filter]\n",
    "#with open(f'pickle/predictions/predictionsf{field}.pk','wb') as f:\n",
    "    #pickle.dump(gc_candidates,f)\n",
    "print(len(gc_candidates))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2008e4cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:\n",
      "Field 35\n",
      "Filtering...\n",
      "427\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "field = 80\n",
    "\n",
    "#fields = [9,21,32,35,37,41,53,56,59,63,73,78,80,79,97,104,103,122,126,121,117,118,135]\n",
    "fields = [35]\n",
    "n_trees = 50\n",
    "max_depth_ = 8\n",
    "max_leaf_nodes_ = 15\n",
    "min_samples_leaf_ = 10\n",
    "features_ = ['i','g','i-g']\n",
    "\n",
    "predictions_list = []\n",
    "print('Start:')\n",
    "for field in fields:\n",
    "    predictions_list = []\n",
    "    gc_filter = []\n",
    "    print(f'Field {field}')\n",
    "    for i in range(50): # iterate to take the intersection of all predictions\n",
    "        predictions_list.append( rf_pred(field,training_data,crowding=250,n_estimators=n_trees,max_depth=max_depth_, max_leaf_nodes=max_leaf_nodes_,min_samples_leaf=min_samples_leaf_, features=features_) )\n",
    "    print('Filtering...')\n",
    "    for i in range(len(predictions_list[0])):\n",
    "        gc_candidate = all([ p[i]['pred']=='gc' for p in predictions_list ]) # select only gcs that were predicted on all iterations\n",
    "        if gc_candidate: gc_filter.append(True)\n",
    "        else: gc_filter.append(False)\n",
    "    gc_candidates = predictions_list[0][gc_filter]\n",
    "    with open(f'pickle/predictions/predictionsf{field}.pk','wb') as f:\n",
    "        pickle.dump(gc_candidates,f)\n",
    "    print(len(gc_candidates))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d66a5e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_filter = []\n",
    "for i in range(len(predictions_list[0])):\n",
    "    gc_candidate = all([ p[i]['pred']=='gc' for p in predictions_list ])\n",
    "    if gc_candidate: gc_filter.append(True)\n",
    "    else: gc_filter.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "7aa9d870",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gc_candidates = predictions_list[0][gc_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f5c2895c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save predictions to pickle file and print how many GCs were found\n",
    "with open(f'pickle/predictionsf{field}.pk','wb') as f:\n",
    "    pickle.dump(gc_candidates,f)\n",
    "len(gc_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767fe7df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
